{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.etl_pbp import extract_files, create_spark_session, get_location_of_ball\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,LongType,IntegerType,FloatType,DateType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.5.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark processing NBA data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc2ae151790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0021500492.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_dwd = ['https://github.com/sealneaward/nba-movement-data/raw/master/data/01.01.2016.CHA.at.TOR.7z']\n",
    "folder_raw = '/workspace/azure_pocket/raw_data' \n",
    "folder_tmp_path = \"/workspace/azure_pocket/raw_data/tmp\" # store json temp\n",
    "\n",
    "game_files = extract_files(folder_raw=folder_raw,folder_tmp_path=folder_tmp_path,urls=urls_dwd)\n",
    "\n",
    "game_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement = get_location_of_ball(folder_tmp_path=folder_tmp_path,game_files=game_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.5.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark processing NBA data</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySpark processing NBA data>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = movement*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# create our schema to upload data\n",
    "schema_location_of_ball_and_teams = StructType([ \\\n",
    "    StructField(\"team_id\",LongType(),True),\n",
    "    StructField(\"player_id\",LongType(),True),\n",
    "    StructField(\"location_x\",FloatType(),True),\n",
    "    StructField(\"location_y\",FloatType(),True),\n",
    "    StructField(\"location_z\", FloatType(),True),\n",
    "    StructField(\"game_clock\", FloatType(),True),\n",
    "    StructField(\"shot_clock\", FloatType(),True),\n",
    "    StructField(\"period\", IntegerType(),True),\n",
    "    StructField(\"game_id\", StringType(),True),\n",
    "    StructField(\"event_id\", StringType(),True),\n",
    "    StructField(\"game_date\", StringType(),True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "teste_df = spark.createDataFrame(teste,schema=schema_location_of_ball_and_teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                (0 + 16) / 16][Stage 2:>                  (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 00:14:18 WARN TaskSetManager: Stage 2 contains a task of very large size (93068 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 00:14:22 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 2 (TID 17): Attempting to kill Python Worker\n",
      "+----------+---------+----------+----------+----------+----------+----------+------+----------+--------+----------+\n",
      "|   team_id|player_id|location_x|location_y|location_z|game_clock|shot_clock|period|   game_id|event_id| game_date|\n",
      "+----------+---------+----------+----------+----------+----------+----------+------+----------+--------+----------+\n",
      "|        -1|       -1|   23.4254|  45.12734|   3.64299|    711.26|     11.99|     1|0021500492|       1|2016-01-01|\n",
      "|1610612761|     2449|  19.08811|  13.91147|       0.0|    711.26|     11.99|     1|0021500492|       1|2016-01-01|\n",
      "+----------+---------+----------+----------+----------+----------+----------+------+----------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "teste_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23253550, 2325355)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(teste),len(movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_df = spark.createDataFrame(movement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/17 00:05:08 WARN TaskSetManager: Stage 0 contains a task of very large size (9309 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-------+------+-----+---+----------+---+----------+\n",
      "|        _1|    _2|      _3|      _4|     _5|    _6|   _7| _8|        _9|_10|       _11|\n",
      "+----------+------+--------+--------+-------+------+-----+---+----------+---+----------+\n",
      "|        -1|    -1| 23.4254|45.12734|3.64299|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|  2449|19.08811|13.91147|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|201960|10.11935|13.54703|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|200768|20.81838|44.51006|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|201942|14.16535| 38.7957|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|202685|20.89057|34.20938|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|101107|13.30744|22.19981|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|201587|  9.1404|18.58649|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|202689|19.31103|42.47285|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|203469|16.44724|32.53167|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|203798|15.49183| 37.7022|    0.0|711.26|11.99|  1|0021500492|  1|2016-01-01|\n",
      "|        -1|    -1| 24.0045| 44.6889|2.99994|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|  2449|18.93523| 13.8225|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|201960| 9.98343|13.71795|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|200768| 21.2291|44.29041|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|201942|13.59663|38.65339|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612761|202685|20.99513|34.48187|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|101107|13.12847|22.14854|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|201587| 8.97947|18.67319|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "|1610612766|202689|19.69412|42.27407|    0.0|711.22|11.97|  1|0021500492|  1|2016-01-01|\n",
      "+----------+------+--------+--------+-------+------+-----+---+----------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "movement_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
